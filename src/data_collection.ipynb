{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect turtle and people counts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "files = []\n",
    "for file in os.listdir(\"../data/hwf_data\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        files.append(os.path.join(\"../data/hwf_data\", file))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data = data.reindex(columns = ['DateTime','TurtleNumber','PeopleNumber','Nr. Volunteers'])\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    df = pd.read_csv(f, header=0)\n",
    "    times = list(df.columns.values)[8:23]\n",
    "    \n",
    "    for i in df['DATE']:\n",
    "        datetimes = pd.Series(pd.to_datetime(pd.Series([i + ' ' + t+'pm' for t in times])),name='DateTime')\n",
    "        t_counts = pd.Series(np.array(df[times][df['DATE']==i].squeeze()),name='TurtleNumber')\n",
    "        p_counts = pd.Series(np.array(df[[t+'.1' for t in times]][df['DATE']==i].squeeze()),name='PeopleNumber')\n",
    "        v_counts = pd.Series([np.array(df.loc[df['DATE']==i,'# of Volunteers '])[0] for t in times],name='Nr. Volunteers')\n",
    "        df_temp = pd.concat([datetimes, pd.to_numeric(t_counts,errors='coerce'), \\\n",
    "                             pd.to_numeric(p_counts,errors='coerce'), \\\n",
    "                             pd.to_numeric(v_counts,errors='coerce')], axis=1)\n",
    "        data = data.append(df_temp)\n",
    "# remove duplicate lines\n",
    "data.drop_duplicates(subset='DateTime',inplace=True)\n",
    "# remove lines where either the turtle or the people counts are nan\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "unique_dates,cnts = np.unique(data['DateTime'],return_counts=True)\n",
    "\n",
    "# chech if duplicate datetimes exist\n",
    "if len(data['DateTime'])!= len(unique_dates):\n",
    "    print('duplicate datetimes exist')\n",
    "    print(len(unique_dates[cnts > 1]))\n",
    "    for i in range(len(unique_dates[cnts > 1])):\n",
    "        print(data[data['DateTime'] == unique_dates[cnts > 1][i]])\n",
    "data.rename(columns={'Nr. Volunteers': 'Nr_Volunteers'}, inplace=True)\n",
    "\n",
    "# sort rows by date\n",
    "data.sort_values(by='DateTime',inplace=True)\n",
    "\n",
    "print(np.shape(data))\n",
    "print(np.min(data['DateTime']),np.max(data['DateTime']))\n",
    "\n",
    "col_names = np.array(list(data.columns))\n",
    "\n",
    "for c in col_names:\n",
    "    print(c,len((data[c].unique())))\n",
    "    print(data[c].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to interpolate columns\n",
    "from scipy.interpolate import interp1d\n",
    "from bisect import bisect\n",
    "\n",
    "def interpolate(x_t,x_p,y_p,max_dt=2):\n",
    "\n",
    "    x_t = np.array(x_t)\n",
    "    x_p = np.array(x_p)\n",
    "    y_p = np.array(y_p)\n",
    "    \n",
    "    sorted_indcs = np.argsort(x_p)\n",
    "    x_p_sorted = x_p[sorted_indcs]\n",
    "    y_p_sorted = y_p[sorted_indcs]\n",
    "    \n",
    "    if np.all([x_p_sorted[i+1] >= x_p_sorted[i] for i in range(len(x_p_sorted)-1)]) == False:\n",
    "        print('sort failed')\n",
    "        raise ValueError\n",
    "    \n",
    "    f = interp1d(x_p_sorted,y_p_sorted,kind='linear')\n",
    "    y_t = []\n",
    "    \n",
    "    for t in x_t:\n",
    "        indx = bisect(x_p_sorted,t)\n",
    "        above = x_p_sorted[indx]\n",
    "        below = x_p_sorted[indx-1]\n",
    "        dt = np.abs(above - below)/3.6e12\n",
    "        if dt <= max_dt:\n",
    "            y_t.append(f(t))\n",
    "        else:\n",
    "            y_t.append(np.nan)\n",
    "    \n",
    "    return np.array(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tide data\n",
    "files = []\n",
    "for file in os.listdir(\"../data/sea_data\"):\n",
    "    if file.endswith(\"tide.csv\"):\n",
    "        files.append(os.path.join(\"../data/sea_data\", file))\n",
    "print(files)\n",
    "\n",
    "tide = pd.DataFrame()\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    df = pd.read_csv(f, header=0)\n",
    "    tide = tide.append(df[['Date Time',' Water Level']])\n",
    "print(np.shape(tide))\n",
    "# remove duplicate rows\n",
    "tide.drop_duplicates(inplace=True)\n",
    "print(np.shape(tide))\n",
    "\n",
    "x_t = pd.to_numeric(pd.to_datetime(data['DateTime']))\n",
    "x_p = pd.to_numeric(pd.to_datetime(tide['Date Time']))\n",
    "\n",
    "new_col = interpolate(x_t,x_p,tide[' Water Level'])\n",
    "data['tide'] = pd.Series(new_col,index=data.index)\n",
    "\n",
    "# add daily mean tide data\n",
    "days = np.array([d[:10] for d in np.array(data['DateTime']).astype(str)])\n",
    "unique_days = np.array(sorted(np.unique(days)))\n",
    "daily_mean_tide = np.zeros(len(unique_days))\n",
    "daily_min_tide = np.zeros(len(unique_days))\n",
    "daily_max_tide = np.zeros(len(unique_days))\n",
    "daily_std_tide = np.zeros(len(unique_days))\n",
    "\n",
    "tide_days = np.array([d[:10] for d in np.array(tide['Date Time']).astype(str)])\n",
    "\n",
    "for i in range(len(unique_days)):\n",
    "    daily_mean_tide[i] = np.mean(tide.loc[tide_days == unique_days[i],' Water Level'])\n",
    "    daily_max_tide[i] = np.max(tide.loc[tide_days == unique_days[i],' Water Level'])\n",
    "    daily_min_tide[i] = np.min(tide.loc[tide_days == unique_days[i],' Water Level'])\n",
    "    daily_std_tide[i] = np.std(tide.loc[tide_days == unique_days[i],' Water Level'])\n",
    "indcs = [np.argwhere(d == unique_days)[0][0] for d in days]\n",
    "data['daily mean tide'] = daily_mean_tide[indcs]\n",
    "data['daily min tide'] = daily_min_tide[indcs]\n",
    "data['daily max tide'] = daily_max_tide[indcs]\n",
    "data['daily stdev tide'] = daily_std_tide[indcs]\n",
    "\n",
    "plt.plot(range(100),daily_mean_tide[:100],label = 'daily mean tide')\n",
    "plt.fill_between(range(100),daily_min_tide[:100],\\\n",
    "                 daily_max_tide[:100],alpha=0.5,label='daily min and max tide')\n",
    "plt.xlabel('days')\n",
    "plt.ylabel('tide height')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(100),daily_std_tide[:100])\n",
    "plt.xlabel('days')\n",
    "plt.ylabel('daily stdev of tide')\n",
    "plt.show()\n",
    "\n",
    "print(len(data['tide'].unique()))\n",
    "\n",
    "col_names = np.array(list(data.columns))\n",
    "\n",
    "for c in col_names:\n",
    "    print(c,len((data[c].unique())))\n",
    "    print(data[c].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add daily tourism data to improve R2 of human counts\n",
    "\n",
    "total = pd.read_excel('../data/tourism_data/daily-pax-update.xls',sheetname=0,header=3,skip_footer=13)\n",
    "domestic = pd.read_excel('../data/tourism_data/daily-pax-update.xls',sheetname=1,header=2,skip_footer=13)\n",
    "international = pd.read_excel('../data/tourism_data/daily-pax-update.xls',sheetname=2,header=0,skip_footer=13)\n",
    "\n",
    "days_total = np.array([d[:10] for d in np.array(total[2018]).astype(str)])\n",
    "total_counts = np.array(total['2018.1'])\n",
    "\n",
    "days_domestic = np.array([d[:10] for d in np.array(domestic[2018]).astype(str)])\n",
    "domestic_counts = np.array(domestic['Unnamed: 4'])\n",
    "\n",
    "days_international = np.array([d[:10] for d in np.array(international['Table 1.1: International Passenger Count excluding from Canada *']).astype(str)])\n",
    "international_total = np.array(international['Unnamed: 2'])\n",
    "international_japan = np.array(international['Unnamed: 3'])\n",
    "international_other = np.array(international['Unnamed: 4'])\n",
    "\n",
    "date_range = np.array([d[:10] for d in pd.date_range(np.sort(days)[0],np.sort(days)[-1]).astype(str)])\n",
    "print(date_range)\n",
    "total_count = np.zeros([len(unique_days),7])\n",
    "domestic_count = np.zeros([len(unique_days),7])\n",
    "int_total_count = np.zeros([len(unique_days),7])\n",
    "int_japan_count = np.zeros([len(unique_days),7])\n",
    "int_other_count = np.zeros([len(unique_days),7])\n",
    "for i in range(len(unique_days)):\n",
    "    print(unique_days[i])\n",
    "    for ii in range(7):\n",
    "        indx = np.argwhere(date_range == unique_days[i])[0][0]\n",
    "        print('   ',date_range[indx-(ii+1)])\n",
    "        total_count[i,ii] = total_counts[days_total == date_range[indx-(ii+1)]][0]\n",
    "        domestic_count[i,ii] = domestic_counts[days_domestic == date_range[indx-(ii+1)]][0]\n",
    "        int_total_count[i,ii] = international_total[days_international == date_range[indx-(ii+1)]][0]\n",
    "        int_japan_count[i,ii] = international_japan[days_international == date_range[indx-(ii+1)]][0]\n",
    "        int_other_count[i,ii] = international_other[days_international == date_range[indx-(ii+1)]][0]\n",
    "        print('   ',total_count[i,ii],domestic_count[i,ii],int_total_count[i,ii],int_japan_count[i,ii],int_other_count[i,ii])\n",
    "    \n",
    "indcs = [np.argwhere(d == unique_days)[0][0] for d in days]\n",
    "for i in range(7):\n",
    "    data['total tourists, '+str(i+1)+' day(s) ago'] = total_count[indcs,i]\n",
    "    data['domestic tourists, '+str(i+1)+' day(s) ago'] = domestic_count[indcs,i]\n",
    "    data['international tourists, '+str(i+1)+' day(s) ago'] = int_total_count[indcs,i]\n",
    "    data['japanese tourists, '+str(i+1)+' day(s) ago'] = int_japan_count[indcs,i]\n",
    "    data['other tourists,'+str(i+1)+' day(s) ago'] = int_other_count[indcs,i]\n",
    "\n",
    "col_names = np.array(list(data.columns))\n",
    "\n",
    "for c in col_names:\n",
    "    print(c,len((data[c].unique())))\n",
    "    print(data[c].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add the weather data\n",
    "weather = pd.read_csv('../data/weather_data/1313747_copy.csv')\n",
    "\n",
    "col_list = list(weather.columns.values)\n",
    "\n",
    "# collect relevant columns, interpolate and add new columns to data\n",
    "\n",
    "cols = ['HOURLYSKYCONDITIONS','HOURLYVISIBILITY', 'HOURLYDRYBULBTEMPC', 'HOURLYWETBULBTEMPC', 'HOURLYDewPointTempC', \\\n",
    "        'HOURLYRelativeHumidity', 'HOURLYWindSpeed', 'HOURLYWindDirection', 'HOURLYWindGustSpeed', \\\n",
    "        'HOURLYStationPressure', 'HOURLYSeaLevelPressure', 'HOURLYPrecip', 'HOURLYAltimeterSetting']\n",
    "\n",
    "x_t = pd.to_numeric(pd.to_datetime(data['DateTime']))\n",
    "x_p = pd.to_numeric(pd.to_datetime(weather['DATE']))\n",
    "    \n",
    "for c in cols:\n",
    "    print(c[6:])\n",
    "    \n",
    "    if c == 'HOURLYSKYCONDITIONS':\n",
    "        conditions = np.array([str(s)[:3] for s in weather[c]])\n",
    "        conds = np.zeros(len(conditions))\n",
    "        conds[conditions == 'CLR'] = 0e0\n",
    "        conds[conditions == 'FEW'] = 1e0\n",
    "        conds[conditions == 'SCT'] = 2e0\n",
    "        conds[conditions == 'BKN'] = 3e0\n",
    "        conds[conditions == 'OVC'] = 4e0\n",
    "        conds[((conditions != 'CLR')&(conditions != 'FEW')&(conditions != 'SCT')&\\\n",
    "                   (conditions != 'BKN')&(conditions != 'OVC'))] = np.nan\n",
    "        new_col = interpolate(x_t,x_p,conds)\n",
    "    else:\n",
    "        new_col = interpolate(x_t,x_p,weather[c])\n",
    "    data[c[6:]] = pd.Series(new_col,index=data.index)\n",
    "    \n",
    "col_names = np.array(list(data.columns))\n",
    "\n",
    "for c in col_names:\n",
    "    print(c,len((data[c].unique())))\n",
    "    print(data[c].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wave data\n",
    "files = []\n",
    "for file in os.listdir(\"../data/sea_data\"):\n",
    "    if file.endswith(\"wave.txt\"):\n",
    "        files.append(os.path.join(\"../data/sea_data\", file))\n",
    "print(files)\n",
    "\n",
    "wave = pd.DataFrame()\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    df = pd.read_table(f, header=0,skiprows=[1],sep='\\s+')\n",
    "    wave = wave.append(df[['#YY','MM','DD','hh','mm','WVHT', 'DPD', 'APD', 'MWD']])\n",
    "print(np.shape(wave))\n",
    "# remove duplicate rows\n",
    "wave.drop_duplicates(inplace=True)\n",
    "print(np.shape(wave))\n",
    "wave.rename(columns={'#YY': 'year', 'MM': 'month','DD': 'day','hh': 'hour','mm': 'minute'}, inplace=True)\n",
    "\n",
    "x_t = pd.to_numeric(pd.to_datetime(data['DateTime']))\n",
    "x_p = pd.to_numeric(pd.to_datetime(wave[['year','month','day','hour','minute']],errors='coerce'))\n",
    "\n",
    "columns = ['WVHT', 'DPD', 'APD', 'MWD']\n",
    "for c in columns:\n",
    "    new_col = interpolate(x_t,x_p,wave[c])\n",
    "    data[c] = pd.Series(new_col,index=data.index)\n",
    "    \n",
    "col_names = np.array(list(data.columns))\n",
    "\n",
    "for c in col_names:\n",
    "    print(c,len((data[c].unique())))\n",
    "    print(data[c].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to csv\n",
    "\n",
    "data.rename(columns={'WVHT': 'Wave Height', 'DPD': 'Dominant Wave Period','APD': 'Average Wave Period','MWD': 'Wave Direction'}, inplace=True)\n",
    "print(data)\n",
    "data.to_csv('../data/combined_dataset.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
